# -*- coding: utf-8 -*-
"""Proyek Pertama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KNgyAVcDl8nd498covw-acl9GYUn3Ton

# Penyusun

*   Nama  : Zhafran Pradistyatama Kuncoro
*   Email : zhafrankuncoro@gmail.com
*   ID DICODING: zhafrankuncoro / MC223D5Y0339
*   Link Dataset: [Dataset Diabetes](https://www.kaggle.com/datasets/marshalpatel3558/diabetes-prediction-dataset-legit-dataset/data)

# 1. Import Library
Pada bagian ini, dilakukan import berbagai library yang akan digunakan dalam proses pengolahan data, eksplorasi, pemodelan, hingga evaluasi model.
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
import numpy as np

"""# 2. Load Dataset"""

df = pd.read_csv('/content/Dataset_Diabetes.csv')

df

"""Terdapat 1000 rows dan 14 Columns

# 3. Data Understanding

## 3.1 Info Dataset
"""

# Menampilkan 5 dataset teratas
df.head()

# Melihat Info dataset
df.info()

# Melihat Deskripsi Statistik Dataset
df.describe()

# Melihat dataset apakah ada missing values
df.isnull().sum()

# Melihat apakah ada data yang duplikat
df.duplicated().sum()

"""insight


*   Missing value: 0
*   Data Duplicated: 0
*   Kolom yang akan dihapus: ID & No_Pation

## 3.2 Univarite Analysis
"""

numerical_features = ['AGE', 'Urea', 'Cr', 'HbA1c', 'Chol', 'TG', 'HDL', 'LDL', 'VLDL', 'BMI']

# Analisis Fitur Numerik
for col in numerical_features:
    if col in df.columns:
        plt.figure(figsize=(12, 5))

        plt.subplot(1, 2, 1)
        sns.histplot(df[col], kde=True, bins=30)
        plt.title(f'Histogram & KDE {col}')
        plt.xlabel(col)
        plt.ylabel('Frekuensi')

        plt.subplot(1, 2, 2)
        sns.boxplot(y=df[col]) # y untuk orientasi vertikal
        plt.title(f'Box Plot {col}')
        plt.ylabel(col)

        plt.tight_layout()
        plt.show()

# Analisis Fitur Kategorikal
categorical_features_eda = ['Gender'] # Kolom 'CLASS' akan di analisis sebagai kolom target
for col in categorical_features_eda:
    if col in df.columns:
        plt.figure(figsize=(8, 5))
        ax = sns.countplot(x=df[col], data=df, palette="pastel", order = df[col].value_counts().index)
        plt.title(f'Distribusi {col}')
        plt.xlabel(col)
        plt.ylabel('Jumlah')
        # Menambahkan persentase pada bar
        total = len(df[col])
        for p in ax.patches:
            percentage = f'{100 * p.get_height() / total:.1f}%'
            x_bar = p.get_x() + p.get_width() / 2
            y_bar = p.get_height()
            ax.annotate(percentage, (x_bar, y_bar), ha='center', va='bottom')
        plt.show()

# nalisis Variabel Target ('CLASS')
target_col = 'CLASS'

if target_col in df.columns:
    plt.figure(figsize=(8, 5))
    ax_target = sns.countplot(x=df[target_col], data=df, palette="Set2", order = df[target_col].value_counts().index)
    plt.title(f'Distribusi Kelas {target_col}')
    plt.xlabel(target_col)
    plt.ylabel('Jumlah')
    total_target = len(df[target_col])
    for p in ax_target.patches:
        percentage_target = f'{100 * p.get_height() / total_target:.1f}%'
        x_bar_target = p.get_x() + p.get_width() / 2
        y_bar_target = p.get_height()
        ax_target.annotate(percentage_target, (x_bar_target, y_bar_target), ha='center', va='bottom')
    plt.show()

"""Insight


*   Bisa dilihat terdapat beberapa outlier yang ada di beberapa kolom numerik, akan tetapi kita tidak akan menghapusnya, karena kita akan menggunakan algoritma Decission Tree dan Random Forest, yang dimana sangat bisa untuk mengatasi outlier. Jika kita menghapus outlier, takutnya akan menghilangkan informasi penting pada data

*   Masih ada kesalahan pada kolom Gender dan CLASS, akan di atasi di Data Preaparation

## 3.3 Multivariate Analysis
"""

# Hubungan Antara Fitur Numerik dan Variabel Target ('CLASS')
if target_col in df.columns:
    for col in numerical_features:
        if col in df.columns:
            plt.figure(figsize=(12, 5))

            plt.subplot(1, 2, 1)
            sns.boxplot(x=df[target_col], y=df[col], palette="viridis")
            plt.title(f'{col} vs. {target_col} (Box Plot)')
            plt.xlabel(target_col)
            plt.ylabel(col)

            plt.subplot(1, 2, 2)
            sns.violinplot(x=df[target_col], y=df[col], palette="plasma", inner="quartile")
            plt.title(f'{col} vs. {target_col} (Violin Plot)')
            plt.xlabel(target_col)
            plt.ylabel(col)

            plt.tight_layout()
            plt.show()

# Hubungan Antara Fitur Kategorikal dan Variabel Target ('CLASS')
if target_col in df.columns:
    for col in categorical_features_eda: # Menggunakan daftar fitur kategorikal dari univariate
        if col in df.columns:
            plt.figure(figsize=(10, 6))
            sns.countplot(x=df[col], hue=df[target_col], data=df, palette="magma")
            plt.title(f'{col} vs. {target_col}')
            plt.xlabel(col)
            plt.ylabel('Jumlah')
            plt.legend(title=target_col)
            plt.show()

# Matriks Korelasi (Hubungan Antar Fitur Numerik)
df_numeric_corr = df[numerical_features].copy()

df_numeric_corr.empty and len(df_numeric_corr.columns) > 1
plt.figure(figsize=(12, 10))
correlation_matrix = df_numeric_corr.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5, annot_kws={"size":8})
plt.title('Matriks Korelasi Fitur Numerik')
plt.show()

"""Insight

*   Terdapat HUbungan dari beberapa data numerik di matriks korelasi
*   Jika kita lihat dari visualisasinya, masih ada kesalahan pada Gender dan CLASS nya kita akan mengatasi di data preparation

# 4. Data Preparation

## Mengatasi permasalahan 1
"""

# Menghapus kolom yang tidak digunakan
df = df.drop(['ID', 'No_Pation'], axis=1)
df.head()

"""## Mengatasi Permasalahan 2"""

df.info()

"""### Mengatasi Kolom Class"""

# Mengatasi Anomali kolom CLASS
df['CLASS'].value_counts()

# mengubah kolom 'CLASS' type string
df['CLASS'] = df['CLASS'].astype(str)


df['CLASS'] = df['CLASS'].str.strip()

#map the values
class_mapping = {
    'N': 'Non-diabetic',
    'P': 'Prediabetic',
    'Y': 'Diabetic'
}

df['CLASS'] = df['CLASS'].map(class_mapping)
print(df['CLASS'].unique())

df['CLASS'] = df['CLASS'].astype('category')

df['CLASS'].value_counts()

"""### Mengatasi kolom Gender"""

df['Gender'].value_counts()

df['Gender'] = df['Gender'].str.strip().str.upper()
df['Gender'].value_counts()

# Mengubah kolom Gender menjadi kategori
df['Gender'] = df['Gender'].astype('category')

df.info()

"""## Encode Kolom Kategorikal"""

# Encodind kolom kategorikal
label_encoder = LabelEncoder()
df['Gender'] = label_encoder.fit_transform(df['Gender'])
df['CLASS'] = label_encoder.fit_transform(df['CLASS'])

"""# 5. Modeling & Evaluasi"""

# Persiapan data untuk modeling
X = df.drop('CLASS', axis=1)
y = df['CLASS']

# Split data train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

"""## Algoritma Decission Tree"""

dt_classifier = DecisionTreeClassifier(random_state=42, max_depth=5)
dt_classifier.fit(X_train, y_train)

# Membuat Prediksi
y_pred = dt_classifier.predict(X_test)

accuracy_score(y_test, y_pred)

print(classification_report(y_test, y_pred))

print(confusion_matrix(y_test, y_pred))

"""## Algoritma Random Forest"""

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

y_pred_rf = rf_model.predict(X_test)
accuracy_score(y_test, y_pred_rf)

print(classification_report(y_test, y_pred_rf))

print(confusion_matrix(y_test, y_pred_rf))

"""## Algoritma Logistic Regression"""

logreg_model = LogisticRegression(random_state=42, max_iter=1000)
logreg_model.fit(X_train, y_train)

y_pred_lr = logreg_model.predict(X_test)
accuracy_score(y_test, y_pred_lr)

print(classification_report(y_test, y_pred_lr))

print(confusion_matrix(y_test, y_pred_lr))

"""## Insight


1.   Performa Terbaik: Decision Tree (DT) dan Random Forest (RF) secara signifikan mengungguli Logistic Regression (LR) dengan akurasi keseluruhan yang identik dan sangat tinggi (~98.67%), sementara LR hanya mencapai ~94.33%.

2.   Keunggulan Model Berbasis Pohon (DT & RF):

*   Sangat efektif dalam mengklasifikasikan semua kelas, termasuk kelas mayoritas (Non-diabetic) dan kelas dengan jumlah sampel menengah (Prediabetic).

*   Untuk Kelas Minoritas (Diabetic - Kelas 2):

  *   Decision Tree (max_depth=5): Unggul dalam menemukan semua kasus aktual Diabetic (Recall 100%).

  *   Random Forest: Unggul dalam memastikan bahwa prediksi Diabetic yang dibuatnya benar-benar akurat (Presisi 100%).

3. Kelemahan Logistic Regression:
* Kurang mampu menangkap pola kompleks dalam data dibandingkan model pohon, yang terlihat dari akurasi yang lebih rendah.
* Sangat kesulitan mengidentifikasi Kelas 2 (Diabetic), dengan tingkat presisi dan recall yang jauh lebih rendah. Ini menunjukkan bahwa batas keputusan linear mungkin tidak cukup untuk memisahkan kelas ini dengan baik. (Perlu diingat, tidak adanya penskalaan fitur mungkin juga berkontribusi pada performa LR yang lebih rendah).

Secara singkat, Decision Tree dan Random Forest adalah pilihan yang sangat baik dan hampir setara untuk dataset ini, sementara Logistic Regression kurang optimal, terutama untuk kelas dengan jumlah sampel sedikit

# Membuat PREDIKSI DENGAN RANDOM DATA
"""

import random

# 1. Sampel Data Acak Anda
# Buat sample dengan Gender string asli
random_sample = pd.DataFrame({
    'Gender': [random.choice(['M', 'F'])],
    'AGE': [np.random.randint(20, 70)],
    'Urea': [np.random.uniform(2.0, 8.0)],
    'Cr': [np.random.randint(30, 100)],
    'HbA1c': [np.random.uniform(3.5, 10.0)],
    'Chol': [np.random.uniform(3.0, 7.0)],
    'TG': [np.random.uniform(0.5, 5.0)],
    'HDL': [np.random.uniform(0.5, 2.0)],
    'LDL': [np.random.uniform(1.0, 5.0)],
    'VLDL': [np.random.uniform(0.1, 1.5)],
    'BMI': [np.random.uniform(18, 35)]
})
print("Sampel Data Acak (Sebelum Encoding Gender):")
print(random_sample)

# Melakukan Encoding pada Gender
random_sample['Gender'] = random_sample['Gender'].map({'M': 1, 'F': 0})
# print("\nSetelah encoding:")
# print(random_sample)
# print("Tipe Gender:", type(random_sample['Gender'][0]))

# Decision Tree
dt_pred = dt_classifier.predict(random_sample)
dt_proba = dt_classifier.predict_proba(random_sample)

# Random Forest
rf_pred = rf_model.predict(random_sample)
rf_proba = rf_model.predict_proba(random_sample)

# Logistic Regression
logreg_pred = logreg_model.predict(random_sample)
logreg_proba = logreg_model.predict_proba(random_sample)

# Mapping hasil prediksi
class_mapping = {0: 'No Diabetes', 1: 'Prediabetes', 2: 'Diabetes'}

# Tampilkan hasil
print("===== PREDIKSI DENGAN BERBAGAI MODEL =====")
print(f"Decision Tree Prediction: {class_mapping[dt_pred[0]]}")
print(f"Decision Tree Probabilities: {dict(zip(class_mapping.values(), dt_proba[0]))}\n")

print(f"Random Forest Prediction: {class_mapping[rf_pred[0]]}")
print(f"Random Forest Probabilities: {dict(zip(class_mapping.values(), rf_proba[0]))}\n")

print(f"Logistic Regression Prediction: {class_mapping[logreg_pred[0]]}")
print(f"Logistic Regression Probabilities: {dict(zip(class_mapping.values(), logreg_proba[0]))}")

"""## Insight

Jadi saya membuat kode ini untuk mendeteksi seorang pasien yang memiliki diabetes dengan pendekatan Machine Learning menggunakan 3 algoritma. Dan saya sudah menjalankan sekitar 10 kali percobaan, dan hasilnya terkadang pada algoritma Logistic regression memiliki Hasil deteksi yang berbeda
"""

